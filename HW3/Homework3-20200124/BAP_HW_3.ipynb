{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before submitting\n",
    "1. Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "2. Make sure that no assertions fail or exceptions occur, otherwise points will be subtracted.\n",
    "\n",
    "4. Please submit only the `*.ipynb` file.\n",
    "\n",
    "5. Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\". Edit only between `YOUR CODE HERE` and `END YOUR CODE`.\n",
    "\n",
    "6. Make sure to use Python 3, not Python 2.\n",
    "\n",
    "Fill your group name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUPNAME = \"\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Dirichlet distribution (5 pts)\n",
    "\n",
    "* Sample $N = 5000$ points from Dirichlet distribution for all given *alphas*, one by one, using **dirichlet** rvs function from _scipy.stats_ package (alredy imported). \n",
    "\n",
    "\n",
    "* Compute pdf values for the samples points with the given alphas one by one using **dirichlet** pdf function.\n",
    "\n",
    "\n",
    "* Perform dimensionality reduction to have 2-dimensional data projections using **PCA** function from _sklearn.decomposition_ package (alredy imported).\n",
    "\n",
    "\n",
    "* Depict your projected data as scatter-subplots using the corresponding pdf values as colors together with **cm.rainbow** colormap.\n",
    "\n",
    "\n",
    "* Set for the scatter plot alpha transparency value equal to 20%.\n",
    "\n",
    "\n",
    "Your plot should look like:\n",
    "\n",
    "<img src='Dir.png' width=700>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import dirichlet, multinomial\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5000\n",
    "alphas = np.array([ [.1,.1,.1],\n",
    "                    [.3,.3,.3],\n",
    "                    [1,1,1],\n",
    "                    [2,1,1],\n",
    "                    [3,1,2],\n",
    "                    [3,3,3]\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,axis = plt.subplots(2, 3, sharex=False, sharey=False, figsize=(16,12))\n",
    "n = 0\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        ax = axis[i,j]\n",
    "        alpha = alphas[n]\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        ax.legend(loc=1)\n",
    "        ax.set_ylim(-0.8,1.2)\n",
    "        ax.set_xlim(-1,1)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        n+=1\n",
    "_=axis[0,1].set_title(\"Dirichlet 2d-projected samples\", fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latex definition section (double click to see)\n",
    "\n",
    "$\\newcommand{\\lrg}{\\large }$\n",
    "$\\newcommand{\\where}{\\ \\text{where}\\ }$\n",
    "$\\newcommand{\\and}{\\ \\text{and}\\ }$\n",
    "$\\newcommand{\\Dir}{\\text{Dir} }$\n",
    "$\\newcommand{\\C}{\\mathcal{C} }$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\E}{\\mathbb{E}}$\n",
    "$\\newcommand{\\const}{\\text{const} }$\n",
    "$\\newcommand{\\sumK}{\\sum_{k=1}^K }$\n",
    "$\\newcommand{\\sumM}{\\sum_{j=1}^M }$\n",
    "$\\newcommand{\\sumV}{\\sum_{v=1}^V }$\n",
    "$\\newcommand{\\sumN}{\\sum_{n=1}^N }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Dircihlet distribution first moment (5 pts)\n",
    "\n",
    "* Write the Dirihlet distribution in expential family form, where pdf of the Dirichlet is given by:\n",
    "\n",
    "\n",
    "$$\\lrg{ \n",
    "p(x | \\alpha) = \\frac{\\Gamma(\\sumK \\alpha_k)}{\\prod_{k=1}^K\\Gamma(\\alpha_k)}\\prod_{k=1}^K x_k^{\\alpha_k-1}, \\where x_k \\in (0,1)\\ \\text{for}\\ k = \\overline{1..K},  \\and \\sumK{x_k} = 1.\n",
    "}$$\n",
    "<br><br>\n",
    "$$\\lrg{ \n",
    "p(x | \\eta) = \\text{YOUR ANSWER HERE}\n",
    "}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using the exponential family property show that the first moment of the Dirichlet distribution is :\n",
    "\n",
    "\n",
    "$$\\lrg{\\E[\\ln\\alpha_k] = \\Psi(\\alpha_k) - \\Psi\\Big(\\sum_{k=1}^K \\alpha_k \\Big)\n",
    "}$$\n",
    "\n",
    "\n",
    "**Property:** $\\lrg{\n",
    "\\E[u(x)] = -\\triangledown_{\\eta} \\ln g(\\eta)  = \\triangledown_{\\eta} A(\\eta)\n",
    "}$\n",
    "\n",
    "**Note:** Digamma function $\\lrg{ \\Psi(x) =  \\frac{d}{dx} \\ln \\Gamma(x) = \\frac{\\Gamma(x)^{'}}{\\Gamma(x)}}$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\lrg{\n",
    "\\E[u(x)] = \\text{YOUR ANSWER HERE}\n",
    "}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3: Multinomial distribution (5 pts)\n",
    "\n",
    "\n",
    "* Draw $M = 6$ samples $\\theta_k \\in \\R^{K},\\ k = \\overline{1..K},\\quad K = 3$ from Dirichlet distribution with alphas as three-dimensional vector of ones.\n",
    "\n",
    "\n",
    "* Use $\\text{random_state} = 42$ for all sampling procedures.\n",
    "\n",
    "\n",
    "* Draw $M = 25$ multinomial samples $X$ for each of the five generated thetas.\n",
    "\n",
    "\n",
    "* Compute the emirical mean over the multinomial samples for each theta and each dimension.\n",
    "\n",
    "\n",
    "* Depict all of your generated thetas as well as the empirical means in one plot with subplots.\n",
    "\n",
    "\n",
    "* Collect all ot the sampled data in $X_{all}$ tensor. $X_{all} \\in \\R^{(M,N,K)}$\n",
    "\n",
    "Your plot should look like:\n",
    "\n",
    "<img src='mult.png' width=1000>\n",
    "\n",
    "<br><br>\n",
    "**Note:** $\\lrg{\\E[x_k] = \\theta_k, \\where x \\sim \\Mult(x | \\theta),\\ x_k \\in \\{0, 1\\},\\ k=\\overline{1..K},\\ \\sumK x_k =1,\\ \\sumK \\theta_k = 1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "M = 6\n",
    "K = 3\n",
    "N = 25\n",
    "alpha_0 = np.ones(K)\n",
    "\n",
    "f,axis = plt.subplots(2, 3, sharex=False, sharey=True, figsize=(16,8))\n",
    "n = 0\n",
    "\n",
    "\n",
    "thetas = # YOUR CODE HERE <- Sample thetas\n",
    "\n",
    "all_X  = []\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        ax = axis[i,j]\n",
    "        space = np.arange(K)\n",
    "        \n",
    "        # YOUR CODE HERE <- Sample x\n",
    "\n",
    "        ax.plot(space, x.mean(0), 'og--',label='Emp mean')\n",
    "        ax.plot(space,thetas[n],'r*--', label='$\\\\mathbb{E}[\\mu] \\sim Mult(\\\\theta)$')\n",
    "        ax.set_xticks(space)\n",
    "        ax.grid(axis='x')\n",
    "        ax.legend(loc=2)\n",
    "        ax.set_xlabel('$\\\\theta = $'+f'{np.round(thetas[n],2)}', fontsize=12)\n",
    "        n+=1\n",
    "axis[0,1].set_title('Multinomial empirical means \\n', fontsize=20)\n",
    "plt.tight_layout()\n",
    "\n",
    "all_X = np.array(all_X)\n",
    "assert all_X.shape == (M,N,K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.4: Multinomial-Dirichlet conjugacy (5 pts).\n",
    "\n",
    "* Evaluate the posterior of $\\theta$ given the model distribution is multinomial with $\\theta$ and prior is Dirichlet with $\\alpha$. See the picture below.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='mdir_conj.png' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model distribution\n",
    "\n",
    "$$\\lrg{\n",
    "p(x_n | \\theta) \\sim \\Mult(x_n | \\theta) ,\\quad \\ln p(x_n | \\theta) = \\sumK x_{nk} \\ln \\theta_{k} + \\const\n",
    "}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior distribution\n",
    "\n",
    "$$\\lrg{\n",
    "p(\\theta | \\alpha) = \\Dir(\\theta | \\alpha), \\quad \\ln p(\\theta | \\alpha) = \\ln\\C(\\alpha) + \\sumK (\\alpha_k - 1) \\ln\\theta_{k}, \\where \\\\ \\quad \\quad \\quad \\quad  \\quad \\quad  \\quad  \\ln\\C(\\alpha) = \\ln\\Gamma\\Big(\\sumK \\alpha_k\\Big) - \\sumK \\ln\\Gamma\\Big(\\alpha_{k}\\Big)\n",
    "}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Evaluate the posterior distribution\n",
    "\n",
    "$$\\lrg{\n",
    "\\ln p(\\theta | X, \\alpha) = \\ln p(X | \\theta) + \\ln p(\\theta | \\alpha) + \\const \n",
    "}$$\n",
    "<br><br>\n",
    "$$\\lrg{\n",
    "\\ln p(\\theta | X, \\alpha) = \\text{YOUR ANSWER HERE} }$$\n",
    "<br><br>\n",
    "$$\\boxed{\\lrg{\n",
    "p(\\theta| X, \\alpha) \\sim \\Dir(\\theta | \\hat\\alpha),\\where \\hat\\alpha = ?\n",
    "}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.5: Dirichlet posterior (3 pts)\n",
    "\n",
    "* Implement the posterior pdf as a function of input data X and hyperparameter $\\alpha$.\n",
    "\n",
    "\n",
    "* Your posterior function should return computed pdf value as well as the alpha_hat.\n",
    "\n",
    "\n",
    "* You may use **dirichlet** function from _scipy.stats_ package for pdf computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior(x, X, alpha):\n",
    "    \n",
    "    # YOUR CODE HERE \n",
    "    \n",
    "    return pdfs, alpha_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.6: Bayes estimation (7 pts)\n",
    "\n",
    "* Compute the Bayes estimator (mean of the posterior) for all of the data generated by corresponding $\\theta$. \n",
    "\n",
    "\n",
    "* Sample $N = 5000$  test data points from uniform-Dirichlet ($\\alpha_0 = 1$) distribution.\n",
    "\n",
    "\n",
    "* Compute posterior pdf values for all of the test points.\n",
    "\n",
    "\n",
    "* Perform dimensionality reduction to 2-dim data with **PCA** function.\n",
    "\n",
    "\n",
    "* Depict the projected test data with corresponding pdfs values as colors. Use **cm.rainbow** colormap.\n",
    "\n",
    "Your plot should look like:\n",
    "\n",
    "<img src='post.png' width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M,N,K = all_X.shape\n",
    "\n",
    "N = 5000\n",
    "alpha_0 = np.ones(K)\n",
    "\n",
    "f,axis = plt.subplots(2, 3, sharex=False, sharey=True, figsize=(16,12))\n",
    "\n",
    "n = 0\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        ax = axis[i,j]\n",
    "        X_tr = all_X[n]\n",
    "        theta = thetas[n]\n",
    "        \n",
    "        pca = PCA(n_components=2)\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        idx = np.argmax(pdf)\n",
    "        ax.set_ylim(-0.8,1.2)\n",
    "        ax.plot(*space_range[idx], 'kx', ms=12, label='$\\\\mathbb{E}[\\\\theta] = $'+f'{E_theta}')\n",
    "        ax.scatter(*space_range.T, c=pdf, cmap=cm.rainbow, alpha=0.2)\n",
    "        ax.set_xlabel('$\\\\theta = $'+f'{np.round(theta,2)}', fontsize=12)\n",
    "        ax.legend(loc=1)\n",
    "        n+=1\n",
    "_=axis[0,1].set_title('2d-proj Dirichlet posterior \\n', fontsize=20)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.7: Evidence function (5 pts)\n",
    "\n",
    "* Evaluate the negative evidence function $-\\mathcal{L}(\\alpha) = -\\ln p(X|\\alpha)$ as a function of the hyperparameter $\\alpha$ and data $X$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\lrg{\n",
    "p(X | \\alpha) = \\int p(X, \\theta|\\alpha)\\ d\\theta = \\int p(X | \\theta) p (\\theta | \\alpha)\\ d\\theta  \n",
    "}$$\n",
    "<br><br>\n",
    "$$\\lrg{\n",
    "-\\ln p(X | \\alpha) =  \\text{YOUR ANSWER HERE} }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Implement the negative **evidence** function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import loggamma\n",
    "def neg_evidence(alpha,X):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return neg_ev "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.8: Bayesian empirical learning (5 pts)\n",
    "\n",
    "* Perform hyperparameter $\\alpha$ minimization using the negative evidence function with _scipy.minimize_ function with the given boundaries.\n",
    "\n",
    "\n",
    "* Collect and print the best (rounded to integer) alphas after minimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "bnds = np.array([[0.01,np.inf]]*K)\n",
    "\n",
    "best_alphas = []\n",
    "for x in all_X:\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    best_alpha = res['x']\n",
    "    best_alphas.append(best_alpha)\n",
    "    \n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.9: Bayesian empirical learning (5 pts)\n",
    "\n",
    "* Repeat procedure from the Exercise 1.6 but now with using best alphas after hyper parameter optimization.\n",
    "\n",
    "\n",
    "* Create the Dirichltet posterior plots similar to the Exercise 1.6\n",
    "\n",
    "\n",
    "* Find the reason why your Bayes estimators are not accurate anymore as compared to them before the empirical optimization.\n",
    "\n",
    "<img src='best_post.png' width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M,N,K = all_X.shape\n",
    "\n",
    "N = 5000\n",
    "alpha_0 = np.ones(K)\n",
    "\n",
    "f,axis = plt.subplots(2, 3, sharex=False, sharey=True, figsize=(16,12))\n",
    "n = 0\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        ax = axis[i,j]\n",
    "        X_tr = all_X[n]\n",
    "        pca = PCA(n_components=2)\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        idx = np.argmax(pdf)\n",
    "        ax.plot(*space_range[idx], 'rx', ms=12, label='$\\\\mathbb{E}[\\\\theta] = $'+f'{E_theta}')\n",
    "        ax.scatter(*space_range.T, c=pdf, cmap=cm.rainbow, alpha=0.2)\n",
    "        ax.set_xlabel('$\\\\theta = $'+f'{np.round(thetas[n],2)}', fontsize=12)\n",
    "        ax.legend(loc=1)\n",
    "        n+=1\n",
    "_=axis[0,1].set_title('2d-proj Dirichlet posterior \\n', fontsize=20)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation (Topic model)\n",
    "\n",
    "\n",
    "Original paper from Dadiv M. Blei could be found <a href='http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf'>here</a>.\n",
    "\n",
    "* Graphical model for the LDA\n",
    "<br>\n",
    "<img src='LDA.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1: LDA paper and true beta loading (5 pts)\n",
    "\n",
    "* Read the paper linked above\n",
    "\n",
    "\n",
    "* Load the _true_ betas from the **true_beta.npy** file. Use _np.load_ function.\n",
    "\n",
    "\n",
    "* Depict the loaded betas as seven 10x10 images using **imshow** function. \n",
    "\n",
    "Your image should look like:\n",
    "\n",
    "<img src='betas.png' width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_BETA = # YOUR CODE HERE\n",
    "\n",
    "K,V = true_BETA.shape\n",
    "f, axis = plt.subplots(1,K)\n",
    "for k in range(K):\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Topic model toy data generation (10 pts)\n",
    "\n",
    "* Implement function called **toy** which generates all of the LDA model parameters: $W$, $z_{true}$ and $\\theta_{true}$ for the given (true) betas. Use $\\alpha_0 = 0.1$ as a defult value.\n",
    "\n",
    "\n",
    "* Input parameters: \n",
    "    \n",
    "    *$M$-number of the generated document\n",
    "    \n",
    "    *$N$-number of the obseved words in each document\n",
    "    \n",
    "    \n",
    "* Use **multinomial** function from _scipy.stats_ package with random seed 42.\n",
    "\n",
    "\n",
    "**Note:** number of topics $K$ and vocabulary size $V$ can be retrieved from the true betas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multinomial, dirichlet\n",
    "\n",
    "def toy(M, N, beta, alpha_0=.1):\n",
    "    K,V = beta.shape\n",
    "    Z = np.empty((M,N,K),dtype=np.int)\n",
    "    W = np.empty((M,N,V),dtype=np.bool)    \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return W,Z,theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Generate all of the LDA model toy data parameters $W$, $z_{true}$ and $\\theta_{true}$ with your function with 128 words in each of the 256 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 256\n",
    "N = 128\n",
    "\n",
    "W, Z_true, theta_true = toy(M, N, true_BETA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3: Log priors for the laten variables $\\theta$  (10 pts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior over document-topic proportions $\\theta = \\Big(\\theta_j\\Big)_{j=1}^M$, where $\\theta_j \\in \\R_{+}^{K}$, such that $\\sum_{k=1}^K \\theta_{jk} = 1$.\n",
    "\n",
    "$$\\lrg{\n",
    "p(\\theta_j | \\alpha_0) \\sim \\Dir(\\theta_j | \\alpha_0) = \\C(\\alpha_0)\\prod_{k=1}^K \\theta_{jk}^{(\\alpha_0 - 1)}, \\quad \\alpha_0 \\in \\R_{+}\n",
    "}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Evaluate the log-prior over topic proportions $\\theta_j$  for the $j$th document with $\\alpha_0$.\n",
    "\n",
    "$$\\lrg{\n",
    "\\ln p(\\theta_j | \\alpha_0) = \\ln \\Dir(\\theta_j | \\alpha_0) = \\ln\\C(\\alpha_0) + \\text{YOUR ANSWER HERE}, \\\\ \\where \\ln\\C(\\alpha_0) = \\text{YOUR ANSWER HERE}\n",
    "}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Evaluate the log-prior for the topic proportions $\\theta$ for all documents with given $\\alpha_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\lrg{\n",
    "\\ln p(\\theta | \\alpha_0) = \\text{YOUR ANSWER HERE}\n",
    "}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior over topic-words proportions $\\beta = \\Big(\\beta_k \\Big)_{k=1}^K$, where $\\beta_k \\in \\R_{+}^{V}$, such that $\\sum_{v=1}^V \\beta_{kv} = 1$\n",
    "\n",
    "$$\\lrg{\n",
    "\\beta_k \\sim \\Dir(\\beta_k | \\eta_0) = \\C(\\eta_0)\\prod_{v=1}^V \\beta_{kv}^{(\\eta_0 - 1)}\n",
    "}$$\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Evaluate the log-prior over topic-words proportions $𝛽_k$ for each topic $k$ given $\\eta_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\lrg{\n",
    "\\ln p(\\beta_k | \\eta_0) = \\ln \\Dir(\\beta_k | \\eta_0) = \\ln\\C(\\eta_0) + \\text{YOUR ANSWER HERE}, \\\\ \\where \\ln\\C(\\eta_0) = \\text{YOUR ANSWER HERE}\n",
    "}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Evaluate the log prior over topic-words proportions $𝛽$ for all topics given $\\eta_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\lrg{\n",
    "\\ln p(\\beta | \\eta_0) = \\text{YOUR ANSWER HERE}\n",
    "}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior over $n$-th word to $k$-th topic assignment in the $j$-th document $z_{jn} \\in \\{0,1\\}^K$, such that $\\sum_{k=1}^K z_{jk} = 1$ is multinomial distribution.\n",
    "\n",
    "$\\newcommand{\\Mult}{\\text{Mult}}$\n",
    "\n",
    "$$\\lrg{\n",
    "z_{jn} \\sim \\Mult(z_{jn} | \\theta_j) = \\prod_{k=1}^K\\theta_{jk}^{z_{jnk}}\n",
    "}$$\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Evaluate the log-prior over $n$-th word to $k$-th topic assignment in the $j$-th document $z_{jn}$ given $\\theta_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\lrg{\n",
    "\\ln p(z_{jn} | \\theta_{j}) = \\text{YOUR ANSWER HERE}\n",
    "}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Evaluate the log-prior over words to topic assignment in all documents $z$ given $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\lrg{\n",
    "\\ln p( z | \\theta)  = \\text{YOUR ANSWER HERE}\n",
    "}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior over the $n$-th word in the $j$-th document $w_{jn} \\in \\{0,1\\}^V$, such that $\\sum_{v=1}^V w_{jnv} = 1$ is m\n",
    "multinomial distribution\n",
    "\n",
    "$$\\lrg{\n",
    "w_{jn} \\sim \\prod_{k=1}^K \\Mult(w_{jn} | \\beta_{k})^{z_{jnk}} = \\prod_{k=1}^K\\bigg(\\prod_{v=1}^V \\beta_{kv}^{w_{jnv}} \\bigg)^{z_{jnk}}\n",
    "}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Evaluate log prior for the $n$-th word in the $j$-th document $w_{jn} \\in \\{0,1\\}^V$ given $z_{jn}$ and $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\lrg{\n",
    "\\ln p( w_{jn} | z_{jn}, \\beta) =  \\sum_{k=1}^K {z_{jnk} \\ln \\Mult(w_{jn} | \\beta_{k})} = \\text{YOUR ANSWER HERE}\n",
    "}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Evaluate log prior for words in all documents given $Z$ and $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\\lrg{\n",
    "\\ln p(w | Z,\\beta) = \\text{YOUR ANSWER HERE}\n",
    "}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.4: Proxy distribution for document to topic proportions (5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given log-joint distribution as following\n",
    "\n",
    "$$\\lrg{\n",
    "\\ln p(w, Z, \\beta, \\theta | \\alpha_0, \\eta_0) = \\ln p(w | Z, \\beta ) + \\ln p(Z |\\theta) + \\ln p(\\theta | \\alpha_0) + \\ln p(\\beta | \\eta_0)\n",
    "}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Evaluate proxy for $\\theta_j$ using the ELBO stationary point condition as a function of $\\theta_j$:\n",
    "\n",
    "$$\\lrg{\n",
    "\\ln q(\\theta_j)^* = \\E_{\\neq \\theta_j}\\Big[\\ln p(Z |\\theta) + \\ln p(\\theta | \\alpha_0) \\Big] + \\const \n",
    "}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\lrg{\n",
    "\\ln q(\\theta_j)^* = \\text{YOUR ANSWER HERE}\n",
    "}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\boxed{\\lrg{\n",
    "q(\\theta_j | \\gamma_j)^* \\sim \\Dir(\\theta_j | \\gamma_j), \\quad \\gamma_{jk} = \\text{YOUR ANSWER HERE}\n",
    "}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\boxed{\\lrg{\n",
    "\\E\\Big[ \\ln \\theta_{jk} \\Big] = \\Psi(\\gamma_{jk}) - \\Psi(\\sumK\\gamma_{jk})\n",
    "}}$$\n",
    "\n",
    "**Note:** Digamma function $\\lrg{ \\Psi(x) =  \\frac{d}{dx} \\ln \\Gamma(x) = \\frac{\\Gamma(x)^{'}}{\\Gamma(x)}}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.5:  Proxy distribution for topic to words proportions $\\beta_k$  (5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Evaluate proxy for topic to words proportions $\\beta_k$  using the ELBO stationary point condition as a function of $\\beta_k$:\n",
    "\n",
    "$$\\lrg{\n",
    "\\ln q(\\beta_k)^* = \\E_{\\neq \\beta_k}\\Big[\\ln p(w | Z, \\beta ) + \\ln p(\\beta | \\eta_0)\\Big] + \\const\n",
    "}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\lrg{\n",
    "\\ln q(\\beta_k)^* = \\text{YOUR ANSWER HERE}\n",
    "}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\boxed{\\lrg{\n",
    "q(\\beta_k | \\hat\\eta_k)^* \\sim \\Dir(\\beta_k | \\hat\\eta_k), \\quad \\hat{\\eta}_{kv} = \\eta_0 + \\sum_{j=1}^M\\sum_{n=1}^N \\E\\Big[z_{jnk}\\Big]w_{jnv}\n",
    "}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\boxed{\\lrg{\n",
    "\\E\\Big[ \\ln \\beta_k \\Big] = \\Psi(\\beta_{kv}) - \\Psi(\\sumV\\beta_{kv})\n",
    "}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.6:  Proxy distribution for word to topic assignment proportions $z_{jn}$  (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Proxy posterior for $z_{jn}$\n",
    "\n",
    "$$\\lrg{\n",
    "\\ln q(z_{jn})^* = \\E_{\\neq z_{jn}} \\Big[\\ln p(w | Z, \\beta ) + \\ln p(Z |\\theta) \\Big] + \\const\n",
    "}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\lrg{\n",
    "\\ln q(z_{jn})^* = \\text{YOUR ANSWER HERE}\n",
    "}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint:** Apply log-sum-exp trick in order to normalize the variational parameter $\\phi_{jnk}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\lrg{\n",
    "\\omega_{jnk} = \\ln \\hat{\\phi}_{jnk} = \\text{YOUR ANSWER HERE}\n",
    "}$$\n",
    "\n",
    "$$\\lrg{ \\phi_{jnk}  = \\frac{\\hat{\\phi}_{jnk}}{\\sumK \\hat\\phi_{jnk}} = \\frac{\\exp(\\ln \\hat{\\phi}_{jnk})}{\\sumK \\exp(\\ln\\hat\\phi_{jnk})} = \\frac{\\exp(\\omega_{jnk})}{\\sumK\\exp(\\omega_{jnk})}\n",
    "}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\lrg{\n",
    "\\phi_{jnk} = \\exp\\Big(\\omega_{jnk}- \\ln\\sumK\\exp(\\omega_{jnk})\\Big)\n",
    "}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\boxed{\\lrg{\n",
    "    q(z_{jn})^* \\sim \\Mult(\\phi_{jn}),\\quad \\sumK \\phi_{jnk} = 1, \\quad \\E[z_{jnk}] = \\phi_{jnk}\n",
    "}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Latent Dirichlet allocation algorithm (10 pts)\n",
    "\n",
    "* Initialize all of the model hyperparameters as well as the required expectations accordingly.\n",
    "\n",
    "\n",
    "* Implement all of the latent variable update steps\n",
    "\n",
    "\n",
    "* Perform ELBO maximization first for the local parameters till convergence ( varible update is tiny e.g. avg diff $< 1e-3$ )\n",
    "\n",
    "\n",
    "* Perform update for the global parameters kiping fixed the local one.\n",
    "\n",
    "\n",
    "* Repeaat the ELBO maximization at least 100 times.\n",
    "\n",
    "\n",
    "* Depict the estimated betas.\n",
    "\n",
    "Your result should look like:\n",
    "\n",
    "<img src='result.png'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL OF YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
